import requests
import os
import time
from datetime import datetime
from sqlalchemy.orm import Session
from database import SessionLocal
from models import Article
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

GNEWS_API_KEY = os.getenv("GNEWS_API_KEY")
BASE_URL = "https://gnews.io/api/v4/search"
LANG = "fr"
MAX = 10  # Nombre d'articles maximum par requ√™te

# ‚úÖ Mots-cl√©s intelligents par cat√©gorie
KEYWORDS_BY_CATEGORY = {
    "Linux": ["linux", "distribution linux", "kernel", "gnu linux"],
    "Syst√®me": ["syst√®me", "infrastructure", "virtualisation", "syst√®me d'exploitation"],
    "Cloud": ["cloud", "aws", "azure", "gcp", "kubernetes", "docker"],
    "R√©seau": ["r√©seau", "networking", "tcp ip", "dns", "routeur", "switch"],
    "S√©curit√©": ["cybers√©curit√©", "faille", "cve", "malware", "attaque", "vuln√©rabilit√© zero day"],
    "D√©veloppement": ["d√©veloppement", "programmation", "python", "javascript", "typescript"],
    "Ev√®nement SI": ["conf√©rence", "webinar", "event", "salon", "rencontre"],
    "Actualit√© Tech": ["technologie", "startup", "innovation", "ia", "chatgpt"]
}


def fetch_articles(query: str):
    params = {
        "q": query,
        "lang": LANG,
        "max": MAX,
        "apikey": GNEWS_API_KEY,
    }
    response = requests.get(BASE_URL, params=params)
    if response.status_code != 200:
        print(f"‚ùå Erreur {response.status_code} pour '{query}' : {response.text}")
        return []
    return response.json().get("articles", [])


def store_articles(articles, category):
    db: Session = SessionLocal()
    total = 0

    for article in articles:
        if db.query(Article).filter(Article.url == article["url"]).first():
            continue  # Article d√©j√† en base

        try:
            db_article = Article(
                title=article["title"],
                url=article["url"],
                content=article.get("content", "")[:1000],
                summary=article.get("description", "")[:300],
                category=category,
                image=article.get("image"),
                published_at=datetime.fromisoformat(article["publishedAt"].replace("Z", "+00:00")),
            )
            db.add(db_article)
            db.commit()
            total += 1
        except Exception as e:
            db.rollback()
            print(f"‚ö†Ô∏è Article ignor√© : {article['title'][:60]}... ‚Üí {e}")

    db.close()
    print(f"üì• {total} nouvel(s) article(s) stock√©(s) pour la cat√©gorie '{category}'.")


def run_scraper():
    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"\nüìÖ Scraper lanc√© √† {now}\n")
    print("üîç Scraping GNews API par cat√©gories intelligentes...\n")

    for category, keywords in KEYWORDS_BY_CATEGORY.items():
        print(f"üìö Cat√©gorie : {category}")
        for keyword in keywords:
            print(f"üîç Recherche : {keyword}...")
            articles = fetch_articles(keyword)
            store_articles(articles, category)
            time.sleep(1)  # √âviter erreur 429

    end = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"\n‚úÖ Scraper termin√© √† {end}\n")


if __name__ == "__main__":
    run_scraper()
